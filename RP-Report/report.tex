\documentclass{report}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{tabu}
\usepackage{color}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{pgf-pie}
\usepackage{pgfplots}
\usepackage{filecontents}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={H20171030081 Report},
    bookmarks=true
}
 
\urlstyle{same}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\graphicspath{ {./images/} }

\begin{document}


\paragraph{}
\begin{center}
\textbf{A Project Report } \\[20pt]
on \\[20pt]


\textbf{\large{\textsl{Internet Wide Scanning and Building Intrusion Detection System Based on the Analysis of Honeypot logs }}} \\[20pt]

by 
\end{center}

\paragraph{}
\begin{center}
\textbf{SARADHI RAMAKRISHNA} \\
2017H1030081H \\
M.E Computer Science
\end{center}

\paragraph{}
\begin{center}
Under the supervision of \\
\textbf{PROFESSOR CHITTARANJAN HOTA} \\
\end{center}

\paragraph{}
\begin{center}
SUBMITTED IN FULFILMENT OF THE REQUIREMENTS OF \\
\textbf{BITS G540 RESEARCH PRACTICE}
\end{center}

\paragraph{}
\begin{center}
\includegraphics[scale=0.1]{bits_logo.png} 
\end{center}

\paragraph{}
\begin{center}
\textbf{BIRLA INSTITUTE OF TECHNOLOGY AND SCIENCE PILANI} \\
\textbf{HYDERABAD CAMPUS} \\
\textbf{(APRIL 2018) }
\end{center}

\thispagestyle{empty}
\newpage





\paragraph{}
\begin{center}
\textbf{\Large{ACKNOWLEDGMENTS}} \\[30pt]
\end{center}
I owe my sincere gratitude to Prof. CHITTARANJAN HOTA, a distinguished faculty at BITS Pilani ,Hyderabad Campus for this project, for his constant
guidance, invaluable suggestions and advice at every stage of the project.


\thispagestyle{empty}
\newpage






\paragraph{}
\begin{center}
\includegraphics[scale=0.1]{bits_logo.png} 
\end{center}

\paragraph{}
\begin{center}
\textbf{BIRLA INSTITUTE OF TECHNOLOGY AND SCIENCE PILANI} \\
\end{center}

\begin{center}
\textbf{HYDERABAD CAMPUS} 
\end{center}

\paragraph{}
\begin{center}
 
\end{center}


\begin{center}
\textbf{\large{Certificate} } \\[20pt]


\end{center}

\begin{flushleft}
This is to certify that the project report entitled “Internet Wide Scanning and Building Intrusion Detection System Based on the Analysis of Honeypot logs” submitted by Mr SARADHI RAMAKRISHNA (ID No. 2017H1030081H) in partial
fulfilment of the requirements of the course BITS G540, Research Practice, embodies
the work done by him under my supervision and guidance.
\end{flushleft}

\paragraph{}
\paragraph{}
\paragraph{}
\paragraph{}
\begin{flushright}
\textbf{(CHITTARANJAN HOTA)} \\[5pt]
BITS- Pilani, Hyderabad Campus
\end{flushright}

\paragraph{}
\begin{flushleft}
Date : 10-04-2018
\end{flushleft}

\thispagestyle{empty}
\newpage







\thispagestyle{empty}
 
\tableofcontents
 
\listoffigures
 
\listoftables

\lstlistoflistings

\newpage
 
\pagenumbering{arabic}

\chapter{INDO-DUTCH Project}


\section{Introduction}
\paragraph{}
I was fortunate enough to be able to work on a budding project under the supervision of Professor Chittaranjan Hota from BITS PILANI and Professor Herbert Bos from VRIJE University Amsterdam. The Project was to determine the availability of all IoT devices on the internet and vulnerabilties of such devices in India and Netherlands. 

\section{Tools}
\paragraph{}
There are many tools available for wider scanning of the internet and finding all the systems which are having specified open ports. Some of the tools are listed below.

\subsection{Shodan}
\paragraph{}
Shodan is a search engine that lets the user find specific types of computers (webcams, routers, servers, etc.) connected to the internet using a variety of filters. Some have also described it as a search engine of service banners, which are metadata that the server sends back to the client. This can be information about the server software, what options the service supports, a welcome message or anything else that the client can find out before interacting with the server.
\begin{center}
\textbf{https://www.shodan.io/explore} 
\end{center}
Above is the link of the search engine where user can query and get the results. Unfortunately it allows only first 10,000 results to be fetched at a time. If a user wants more than that , they have to send a request to shodan team for getting access to more than 10K records.

\paragraph{}
Shodan gives the most of metadata using which we can identify what type of device it is and what are the open ports.Shodan collects data mostly on web servers (HTTP/HTTPS - port 80, 8080, 443, 8443), as well as FTP (port 21), SSH (port 22), Telnet (port 23), SNMP (port 161), IMAP (port 993), SIP (port 5060), and Real Time Streaming Protocol (RTSP, port 554). The latter can be used to access webcams and their video stream.It also has REST API support which can be easily integrated with other applications.

\subsection{Nmap}
Nmap (Network Mapper) is a security scanner, used to discover hosts and services on a computer network, thus building a "map" of the network. To accomplish its goal, Nmap sends specially crafted packets to the target host(s) and then analyzes the responses.The software provides a number of features for probing computer networks, including host discovery and service and operating-system detection. These features are extensible by scripts that provide more advanced service detection,vulnerability detection,and other features. Nmap can adapt to network conditions including latency and congestion during a scan.

\paragraph{}
Nmap features include:
\begin{itemize}
\item Host discovery
\item Port scanning 
\item Version detection
\item OS detection
\end{itemize}

\subsection{ZMap}
\paragraph{}
ZMap is a fast single packet network scanner designed for Internet-wide network surveys. On a computer with a gigabit connection, ZMap can scan the entire public IPv4 address space in under 45 minutes. 

\paragraph{}
ZMap operates on GNU/Linux, Mac OS, and BSD. ZMap currently has fully implemented probe modules for TCP SYN scans, ICMP, DNS queries, UPnP, BACNET, and can send a large number of UDP probes.

\paragraph{}
ZMap also provides online platform for searching the IPV4 address space. It is called Censys. It is useful when we are having limitation on computational capability and network reachability.

\subsection{Censys}
\paragraph{}
Censys is a platform that helps information security practitioners discover, monitor, and analyze devices that are accessible from the Internet. It regularly probe every public IP address and popular domain names, curate and enrich the resulting data, and make it intelligible through an interactive search engine. Censys also provides API's using which any user can interact with their database. Online search engine link is given below.

\begin{center}
\textit{https://censys.io/}
\end{center}

\paragraph{}
After carefully analyzing all the tools , their functionalities and their limitations, it has beed decided to go with Censys Search engine provided by ZMap project team. But it has a limitation that a normal user can only get access to first 10,000 results of their database.

\paragraph{}
So, Mails are sent to Censys team and necessary privileges have been granted for us. Using the rest API's , we were able to access only first 10,000 results. So they provided access to their central database using Google BigQuery platform.

\subsection{Google BigQuery}
\paragraph{}
BigQuery is Google's serverless, highly scalable, low cost enterprise data warehouse designed to make all your data analysts productive. BigQuery enables us to analyze all our data by creating a logical data warehouse over managed, columnar storage as well as data from object storage, and spreadsheets.

\begin{figure}[h!]
\centering
\caption{Google BigQuery Engine}
\includegraphics[scale=0.2]{google_big_query}
\end{figure}

\paragraph{}
Google BigQuery engine looks like above. After we got access to their entire database, we started running queries to get the data of India and Netherlands entire database of all open devices and their ports.

\paragraph{}
India and Netherlands databases are huge in size comprising around 40 to 50 GB. We saved them as tables in Google BigQuery engine. Later we exported them to our local Bits Server machines in PostgreSQL databases for running our queries locally. Below are some of the statistics.


\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 \textbf{DEVICE-TYPE} & \textbf{COUNT OF DEVICES}\\
 \hline
 nas (Network Access Storage) & 73247  \\
  \hline
 soho router & 35951  \\
  \hline
 IPMI & 12039  \\
  \hline
 infrastructure router & 11315  \\
  \hline
 network & 7388  \\
  \hline
 cable modem & 3572  \\
  \hline
 DSL/cable modem & 3076  \\
  \hline
 camera	 & 2007  \\
  \hline
 printer & 1437  \\
  \hline
 power distribution unit & 1110  \\
  \hline
 DSL modem & 698  \\
 \hline
 firewall & 644  \\
 \hline
 scada controller & 531  \\
 \hline
 alarm system & 225  \\
 \hline
 kvm & 103  \\
 \hline
 switch & 83  \\
 \hline
 set-top box & 59  \\
 \hline
 scada gateway & 57  \\
 \hline
 programmable logic controller	 & 25  \\
 \hline
 scada server & 21  \\
 \hline
 solar panel & 15  \\
 \hline
 hvac & 14  \\
 \hline
 DVR & 12  \\
 \hline
 laser printer & 8  \\
 \hline
 environment monitor & 5  \\
 \hline
 scada router & 5  \\
 \hline
\end{tabular}
\caption{List of Devices in Netherlands}
\end{center}
\end{table}

\paragraph{}
There were a total of close to 1.6 million open devices in Netherlands which is a huge number and can be taken advantage of which can cause severe damage. Python code has been written for getting all the devices in India and Netherlands. 

\lstinputlisting[language=python, caption=All open devices in India and Netherlands, captionpos=top]{./files/1_Get_Ports.py}

\paragraph{}
The total number of ports and their counts is also found out by running the queries using python. It is given below.

\lstinputlisting[language=python, caption=All open ports and their counts, captionpos=top]{./files/2_GetSpecificPortCount.py}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 \textbf{DEVICE-TYPE} & \textbf{COUNT OF DEVICES}\\
 \hline
 network & 135522  \\
  \hline
 infrastructure router & 43909  \\
  \hline
 soho router & 10268  \\
  \hline
 camera & 9491  \\
  \hline
 DSL/cable modem & 3356  \\
  \hline
 nas & 2537  \\
  \hline
 IPMI & 1135  \\
  \hline
 printer	 & 931  \\
  \hline
 firewall & 667  \\
  \hline
 DSL modem & 298  \\
  \hline
 power distribution unit & 242  \\
 \hline
 DVR & 182  \\
 \hline
 kvm & 50  \\
 \hline
 environment monitor	 & 24  \\
 \hline
 solar panel & 13  \\
 \hline
 scada server	 & 10  \\
 \hline
 switch & 9  \\
 \hline
 scada router & 9  \\
 \hline
 cable modem	 & 8  \\
 \hline
 set-top box & 8  \\
 \hline
 wireless modem & 8  \\
 \hline
 scada controller & 6  \\
 \hline
 scada gateway & 2  \\
 \hline
\end{tabular}
\caption{List of Devices in India}
\end{center}
\end{table}

\paragraph{}
There were close to 2 Million devices in India. Open ports and their counts are given below for both Netherlands and India.


\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|||||c|c| } 
 \hline
 \textbf{PORT} & \textbf{COUNT} & \textbf{PORT} & \textbf{COUNT}\\
 \hline
 443 & 2670606  & 80 & 1925425\\
  \hline

22 & 409455  & 25 & 262318\\
  \hline
 
 21 & 218763 & 143 & 192846   \\
  \hline

 110 & 177016 & 995	 & 174642 \\
  \hline
 
 8080 & 171372 &  993 & 162578 \\
  \hline
 
 53 & 134094 & 465 & 108951 \\
 \hline
 
 587 & 45178 & 23	 & 28813 \\
 \hline

 8888 & 24090  & 445	 & 5637\\
 \hline

 7547 & 2545  & 2323 & 2257 \\
 \hline
 
 1911 & 931  & 1900 & 647 \\
 \hline
 
 502 & 347  & 102 & 107 \\
 \hline
 
 47808 & 75  & 20000 & 1 \\
 \hline

\end{tabular}
\caption{List of Ports and their counts in Netherlands}
\end{center}
\end{table}



\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|||||c|c| } 
 \hline
 \textbf{PORT} & \textbf{COUNT} & \textbf{PORT} & \textbf{COUNT}\\
 \hline
 443 & 1427660  & 80 & 683484\\
  \hline

8080 & 263629  & 22 & 184335\\
  \hline
 
 53 & 169409 & 8888 & 160570   \\
  \hline

 110 & 103431 & 21	 & 98719 \\
  \hline
 
 25 & 90601 &  143 & 84316 \\
  \hline
 
 23 & 74346 & 995 & 59108 \\
 \hline
 
 993 & 49866 & 465	 & 39585 \\
 \hline

 445 & 39177  & 587	 & 9668\\
 \hline

 7547 & 7810  & 1900 & 2197 \\
 \hline
 
  2323 & 1048  & 502 & 106 \\
 \hline
 
  47808 & 11  & 102 & 7 \\
 \hline
 
  1911 & 2  & 20000 & 1 \\
 \hline

\end{tabular}
\caption{List of Ports and their counts in India}
\end{center}
\end{table}

\paragraph{}
There were in total of 35 Million devices in India and 67 Million devices in Netherlands. 2 tables were created in the PostgreSQL database for devices in India and Netherlands and queries were run against those tables. Below is the command for connecting to database "censys".

\begin{center}
\textit{ sudo -u postgres psql -d censys}
\end{center}

\paragraph{}
After they are satisfied with our results, entire database has been handed to them for future work.


\chapter{Analysis and Design of IDS using Honeypots}

\section{Introduction of Tools}

\paragraph{}
In this section, i will be introducing the tools ,languages and API's which will be used throughout the report.

\subsection{ELK Stack}
\paragraph{}
"ELK" is the acronym for three open source projects: Elasticsearch, Logstash, and Kibana.

\subsubsection{Elastic Search}

\paragraph{}
Elasticsearch is a search engine based on Lucene. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.

\subsubsection{Logstash}

\paragraph{}
Logstash is an open source tool for collecting, parsing, and storing logs for future use.

\subsubsection{Kibana}

\paragraph{}
Kibana is an open source data visualization plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.

\subsection{Honeypots}

\paragraph{}
In computer terminology, a honeypot is a trap set to detect, deflect or in some manner counteract attempts at unauthorized use of information systems.

\paragraph{}
In other words, A server that is configured to detect an intruder by mirroring a real production system. It appears as an ordinary server doing work, but all the data and transactions are phony. Located either in or outside the firewall, these are used to learn about an intruder’s techniques as well as determine vulnerabilities in the real system.”

\paragraph{}
Based on design criteria, honeypots can be classified as

\begin{enumerate}
\item Low-interaction honeypots
\item Medium-interaction honeypots
\item High-interaction honeypots
\end{enumerate}

\subsubsection{Low-interaction honeypots}
\paragraph{}
Low-interaction honeypots simulate only the services frequently requested by attackers. Since they consume relatively few resources, multiple virtual machines can easily be hosted on one physical system, the virtual systems have a short response time, and less code is required, reducing the complexity of the security of the virtual systems.
\paragraph{}
Low-interaction honeypots present the hacker emulated services with a limited subset of the functionality they would expect from a server, with the intent of detecting sources of unauthorized activity. For example, the HTTP service on low-interaction honeypots would only support the commands needed to identify that a known exploit is being attempted.

\subsubsection{Medium-interaction honeypots}
\paragraph{}
Medium-interaction honeypots  might more fully implement the HTTP protocol to emulate a well-known vendor’s implementation, such as Apache. However, there are no implementations of a medium-interaction honeypots and for the purposes of this paper, the definition of low-interaction honeypots captures the functionality of medium-interaction honeypots in that they only provide partial implementation of services and do not allow typical, full interaction with the system as high-interaction honeypots.

\subsubsection{High-interaction honeypots}
\paragraph{}
High-interaction honeypots imitate the activities of the real systems that host a variety of services. It let the hacker interact with the system as they would any regular operating system, with the goal of capturing the maximum amount of information on the attacker’s techniques. Any command or application an end-user would expect to be installed is available and generally, there is little to no restriction placed on what the hacker can do once he/she comprises the system.
\paragraph{}
According to recent researches in high interaction honeypot technology, by employing virtual machines, multiple honeypots can be hosted on a single physical machine. Therefore, even if the honeypot is compromised, it can be restored more quickly. Although high interaction honeypots provide more security by being difficult to detect, but it has the main drawback that it is costly to maintain.

\subsubsection{Cowrie Honeypot}
\paragraph{}
Cowrie is used for our research purposes.Cowrie is a medium interaction SSH and Telnet honeypot designed to log brute force attacks and the shell interaction performed by the attacker.
\paragraph{Interesting Features of Cowrie Honeypot}

\begin{enumerate}
\item Fake filesystem with the ability to add/remove files. A full fake filesystem resembling a Debian 5.0 installation is included.
\item Possibility of adding fake file contents so the attacker can cat files such as /etc/passwd. Only minimal file contents are included.
\item Logging in JSON format for easy processing in log management solutions.
\end{enumerate}

\subsection{VirusTotal}
\paragraph{}
VirusTotal is a website created by the Spanish security company Hispasec Sistemas. Launched in June 2004, it was acquired by Google Inc. in September 2012. The company's ownership switched in January 2018 to Chronicle, a subsidiary of Alphabet Inc. (Google's parent company).

\paragraph{}
VirusTotal aggregates many antivirus products and online scan engines to check for viruses that the user's own antivirus may have missed, or to verify against any false positives.Files up to 256 MB can be uploaded to the website or sent via email.Anti-virus software vendors can receive copies of files that were flagged by other scans but passed by their own engine, to help improve their software and, by extension, VirusTotal's own capability. 

\subsubsection{List of Anti-Virus Engines used by VirusTotal}


\begin{table}[h!]
\begin{center}

\begin{tabular}{ |c|c|c|c| } 
 \hline
 AegisLab & Agnitum & AhnLab & Anity\\
 \hline
 Aladdin & Avast & AVG  & Avira \\
 \hline
 BluePex  & Baidu  & BitDefender & Bkav  \\
 \hline
 ByteHero & Quick Heal & CMC Antivirus & CYREN \\
 \hline
 ClamAV  & Comodo & CrowdStrike & Doctor Web Ltd. \\
 \hline
 Emsisoft & Endgame & Eset Software & Fortinet \\
 \hline
 F-Prot  & F-Secure & G Data & Hacksoft \\
 \hline
 Hauri  & IKARUS & nProtect & Invincea  \\
 \hline
 Jiangmin & K7AntiVirus & Kaspersky & Kingsoft\\
 \hline
 Malwarebytes  & McAfee & Microsoft & eScan \\
 \hline
 Nano Security  & Norman & Panda & Rising \\
 \hline
  Symantec  & VIPRE & TotalDefense & TrendMicro \\
 \hline
 
 
 \hline
\end{tabular}
\caption{List of Antivirus Engines}
\end{center}
\end{table}

\subsection{Github}
\paragraph{}
GitHub (originally known as Logical Awesome LLC) is a web-based hosting service for version control using git. It is mostly used for computer code. It offers all of the distributed version control and source code management (SCM) functionality of Git as well as adding its own features. It provides access control and several collaboration features such as bug tracking, feature requests, task management, and wikis for every project.

\paragraph{}
The code which is written for this project is entirely pushed to Github where it is bug free and version safe.

\section{Installation of Tools}
\paragraph{}
In this section, we will be mentioning about the installation of various tools which will be used in the project.

\subsection{Elastic Search}

\begin{enumerate}
\item Download the Debian Installation file from the below link \\
\textit{\href{https://www.elastic.co/downloads/elasticsearch}}
\item Install it using the below command \\
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:~} dpkg -i elastic-search-6.2.0.deb}
\item Now start the service using below command \\
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:~} sudo service elasticsearch start}
\end{enumerate}

\paragraph{}
Open the link \textit{http://localhost:9200} in the browser and check for the ouput which should look like below.

\begin{figure}[H]
\centering
\caption{Elastic Search Output}
\includegraphics[scale=0.7]{Elastic_Search_Installation}
\end{figure}

\paragraph{}
We can check for the working of elastic search in the browser by typing the url \textit{http://localhost:9200} and verifying the output.

\subsection{Logstash}

\begin{enumerate}
\item Download the logstash debian installation file from below link\\
\textit{https://www.elastic.co/downloads/logstash}
\item Install it using the below command \\
\textit {{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:~} dpkg -i logstash-6.2.2.deb}
\item Now start the service using below command\\
\textit{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:~} sudo service logstash start
\end{enumerate}

\subsection{Kibana}

\begin{enumerate}
\item Download the Kibana debian installation file from below link\\
\textit{https://www.elastic.co/downloads/kibana}
\item Install it using the below command\\
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:~} dpkg -i kibana-6.2.2.deb}
\item Now start the service using below command\\
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:~} sudo service kibana start}
\end{enumerate}



\begin{figure}[H]
\centering
\caption{Kibana Dashboard}
\includegraphics[scale=0.22]{Kibana_Installation}
\end{figure}

\paragraph{}
Open the link (\textit{http://localhost:5601}) in the browser and check for the Kibana Dashboard which looks like above.


\subsection{Cowrie Honeypot}

\paragraph{}
Cowrie honeypot is installed in a separate machine with public IP which is connected to the internet.It can be installed by following the steps given below.

\subsubsection{Install dependencies } 
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} sudo apt-get install git python-virtualenv libssl-dev libffi-dev build-essential libpython-dev python2.7-minimal authbind}

\subsubsection{Create a user account } 
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} sudo adduser --disabled-password cowrie}\\
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} sudo su - cowrie}

\subsubsection{Checkout the code } 
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} git clone http://github.com/micheloosterhof/cowrie}\\
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} cd cowrie}


\subsubsection{Setup Virtual Environment } 
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} pwd}\\
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} virtualenv cowrie-env}

\subsubsection{Port redirection } 
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} sudo iptables -t nat -A PREROUTING -p tcp --dport 22 -j REDIRECT --to-port 2222}

\paragraph{}
In the same folder, create a copy of \textit{cowrie.cfg.dist} and modify the file according to your needs like changing the name, SSH port number etc. 

\subsubsection{Starting Cowrie }
\textit{{\color{blue}iotsys3@iotsys3-Precision-Tower-3420:} bin/cowrie start}

\paragraph{}
Cowrie will be up and running on port 22 which will monitor all SSH logins and commands executed by the attackers or intruders.


\section{Working Model}

All the tools listed above are integrated to form a complete Working Model which looks like below.

\begin{figure}[H]
\centering
\caption{Working Model}
\includegraphics[scale=0.55]{Working_Model}
\end{figure}

\section{Monitoring System}

\begin{figure}[H]
\centering
\caption{Monitoring System}
\includegraphics[scale=0.55]{Monitoring_System}
\end{figure}

\paragraph{}
Cowrie will be generating the logs in the form of JSON format which will be available in public IP machine as specified in the figure-3. In order to stream the logs from the public IP machine to Server machine, filebeat can be used which is an open source tool. Unfortunately we did not get to use that tool as there was no physical connection between public IP machine and server machine due to security reasons. 

\paragraph{}
So all the logs are copied from public IP machine to server machine on daily basis. These log files are supplied as input to Logstash which will parse, add metadata and push them to Elastic Search database. Logstash works on config files. Each config file has three parameters namely input, filter and output. Our config file is given below.

\lstinputlisting[language=python, caption=Cowrie Config File, captionpos=top]{cowrie.conf}

\begin{enumerate}
\item Input
\paragraph{}
It contains a parameter file which has options path, start-position and ignore-older. Path is where log files are present and start-position indicates from which point Logstash should start reading the logs from.
\item Filter
\paragraph{}
Filter is where entire processing happens in logstash. It has many options out of which json option is used as the logs are in JSON format. Some more information is added to the already present logs like country , Continent , latitude and longitude information which are specified in geoip option. Finally latitude and longitude are converted into float variables for plotting on map.
\item Output
\paragraph{}
It specifies options as to where the data should go after the processing is done. There are many options out of which we are using elastc search where data is stored in json format which will be in key value format.
\end{enumerate}

\paragraph{}
Data collected in the elastic search will be picked up by the Kibana by default as specified in the Kibana config files. Using the data visualizations are created for analyzing and monitoring the regular changes in data. Monitoring system was deployed for 40 days and collected nearly \textbf{2 Million Records}.

\paragraph{}
Using Kibana different visualizations are created which are listed below.
\begin{enumerate}
\item Daily Attacks Count Area Graph
\paragraph{}
An area graph was plotted against the count of attacks on daily basis. X-axis represents the date and Y-axis represents the count of attacks.It can observed that, attacks kept on increasing at the end.

\begin{figure}[H]
\centering
\caption{Daily Attacks Count Area Graph}
\includegraphics[scale=0.2]{Daily_Attacks_Count}
\end{figure}


\item Top 20 Usernames Histogram
\paragraph{}
A Histogram was used to represent the top 20 usernames that have been used by the attackers and intruders.

\begin{figure}[H]
\centering
\caption{Top 20 Usernames Histogram}
\includegraphics[scale=0.2]{Username_Histogram}
\end{figure}

\paragraph{}
From the graph, it can be observed that usernames "admin", "root and "ubnt" are mostly used by attackers.

\item Unique Count Of IP's
\paragraph{}
A Kibana Utility graph named Unique Count has been used for counting the Unique Number of IP's till now monitoring system has captured. It has found to be nearly 2000 IP's.

\begin{figure}[H]
\centering
\caption{Unique Count of IP's}
\includegraphics[scale=0.5]{Unique_IP}
\end{figure}

\item Top 10 Passwords Pie
\paragraph{}
A pie chart has been drawn against the top 10 passwords used by the attackers and intruders. It can be observed that passwords "ubnt" and "admin" are mostly used.

\paragraph{}
\begin{tikzpicture}
\centering
 \pie [rotate = 90,sum = auto]
    {1465/admin,
     1242/1234,
     1118/ubnt,
     883/root, 
     742/password,
     547/user,
     536/123456,
     497/1111,
     399/support,
     355/TestingR2}
\end{tikzpicture}

\item Global Attacks Map
\paragraph{}
A map is also plotted against the attacks receiving from different contries and continents using the latitude and longitude values. Most of the attacks are received from european continent and china country.

\begin{figure}[H]
\centering
\caption{Global Attacks Map}
\includegraphics[scale=0.30]{Global_Attacks_Map}
\end{figure}

\end{enumerate}

\subsection{Stochastic Process}
\paragraph{}
Stochastic process is also called random process. It contains random variables with probabilities associated to it. While exact path cannot be inferred, assumptions can be made based on the probabilities of random variables. Attacks pattern observed is following random process or stochastic process as number of attacks on daily basis is not following any pattern.

\begin{figure}[H]
\centering
\caption{Stochastic Process}
\includegraphics[scale=0.7]{Stochastic_Process}
\end{figure}

\section{Intrusion Detection System}
\paragraph{}
An Intrusion Detection System is a software application which regularly monitors the network for malicious activities and reports them to central authority for fail-safe measures. After the monitoring system is setup and data is collected , task is now to design an Intelligent Intrusion Detection System which applies machine learning algorithms on the data and comes up with firewall rules to block malicious activites and IP's. For designing, tools like VirusTotal API's, PostgreSQL database, Spark K-Means Algorithm etc. are used which is given below in detail.

\paragraph{}
Virus Total is a website where details about different malwares and IP's can be fetched by using their web interface or REST API's. For our convenience, API's are used to integrate with our python programs. Virus Total gives a normal user on an average of 1000 API requests per day combining malware samples and IP's. We have sent a mail to VirusTotal requesting Research User  privileges which has access to 10,000 API requests per day and all malware samples present in their database and got their approval.

\paragraph{}
In PostgreSQL, database named "cowrie" has been created and below tables were created to store the required data from Elastic Search and VirusTotal.

\begin{table}


\begin{tabular}{ |p{2cm}|p{5cm}|p{5cm}|  }
 \hline
 \textbf{S.No.} & \textbf{Table Name} & \textbf{Schema}\\
 \hline
 \hline
 1 & Dos-Attacks & public \\
 \hline
2 & Malicious-URLs & public \\
\hline
3 & Stats & public \\
\hline
\end{tabular}
 \caption{List of Tables in PostgreSQL Database}
\end{table}




\begin{figure}[H]
\centering
\caption{Intrusion Detection System}
\includegraphics[scale=0.7]{Intrusion_Detection_System}
\end{figure}

\subsection{Malicious-URLs Table}

\begin{table}[H]
\begin{tabular}{ |p{2cm}|p{5cm}|p{5cm}|  }
 \hline
 \textbf{S.No.} & \textbf{Column Name} & \textbf{Column Datatype}\\
 \hline
 \hline
 1 & url & text \\
 \hline
2 & scan-url & text \\
\hline
3 & positives & integer \\
\hline
3 & total & integer \\
\hline
3 & identified-engines & text \\
\hline
\end{tabular}
 \caption{Schema of Malicious-URLs Table}
\end{table}

\paragraph{}
Column "url" is used to store the malicious url and "scan-url" is the ID to identify the job submitted to VirusTotal for malicious url. "positives" contains the number of antivirus engines identified the url as malicious and   "total" indicated the number of antivirus engines present in VirusTotal. "identified-engines" contains the names of all anti-virus engines. Malicious-urls table gets the data from VirusTotal API's. Python codes and their explanations are given below.

 
\lstinputlisting[language=python, caption=Get Malicious URLs From Virus Total, captionpos=top]{./files/12-GetMaliciousURLsFromIPsUsingVirusTotal.py}

 
\lstinputlisting[language=python, caption=Add URL's to PostgreSQL Database, captionpos=top]{./files/13-AddURLsToDB.py}

 
\lstinputlisting[language=python, caption=Submit URL's for Scan to VirusTotal, captionpos=top]{./files/14-SubmitURLsForScanUsingVirusTotal.py}

 
\lstinputlisting[language=python, caption=Get URL Scan results from VirusTotal, captionpos=top]{./files/15-GetURLScanResults.py}


\paragraph{}
Above python codes uses Virus Total API and sends requests for all the malicious urls identified for the specific IP. The response is decoded and then stored into the table. response looks like below.

\lstinputlisting[language=python, caption=Virus Total API Request Sample, captionpos=top]{./files/response.txt}

\paragraph{}
This response is parsed and requires fields are extracted to update the table in the database. These fields include "positives" which is the no of engines identified it as positive, "total" which is the count of the engines in total  and "identified-engines" which contains the names of all the engines in the response.


\subsection{Stats Table}

\begin{table}[H]
\begin{tabular}{ |p{2cm}|p{5cm}|p{5cm}|  }
 \hline
 \textbf{S.No.} & \textbf{Column Name} & \textbf{Column Datatype}\\
 \hline
 \hline
 1 & ip & character varying(20) \\
 \hline
2 & commands & text \\
\hline
3 & countofcommands & integer \\
\hline
4 & loginattempts & integer \\
\hline
5 & doscluster & text \\
\hline
6 & sentiment & integer \\
\hline
\end{tabular}
 \caption{Schema of Stats Table}
\end{table}

 
\paragraph{}
Column "IP" contains the ip of the attacker or intruder and "commands" contains all the commands executed by the attacker or the intruder till noe in cowrie honeypot. "Count-of-commands" contains the number of commands attacker executed. "login-attempts" contains the number of times attacker tried to login into the cowrie. "dos-cluster" contains the cluster number to which ip is clustered which will be updated dynamically. "sentiment" contains the value of ip which is updated after running machine learning algorithms.

\paragraph{}
All the python codes used for filling the table are given below one by one.


\lstinputlisting[language=python, caption=Get Unique IPs from Elastic Search, captionpos=top]{./files/01_GetUniqueIPs.py}

\paragraph{}
The above python code fetches all the IP's from elastic search, puts them in a set to avoid duplicates and copies all the IP's to the column named "ip" in Stats table. It uses the Elastic Search API's which looks like below.

\begin{lstlisting}[language=Python, caption=Elastic Search API Request Sample,captionpos=top]
POST localhost:9200/logstash-*/_search
{
  "size" : 0,
    "aggs" : {
        "distinct_ip" : {
            "cardinality" : {
              "field" : "geoip.ip"
            }
          
        }
    }
}
\end{lstlisting}


\lstinputlisting[language=python, caption=Get Commands Executed by IPs from Elastic Search, captionpos=top]{./files/02_GetCommandsForIPs.py}

\paragraph{}
The above python code fetches all the commands executed per IP , appends them together separated by semicolon and puts them in the column named "commands" in Stats table. It also updates the "countofcommands" column.

\lstinputlisting[language=python, caption=Get Login Attempts by IPs from Elastic Search, captionpos=top]{./files/03_getLoginAttemptsForIPs.py}

\paragraph{}
The above python code gets the number of login attempts done by each IP from the elastic search. It counts the attempts and stores them in column named "loginattempts" in Stats table. 

\subsubsection{DOS Attacks}
\paragraph{}
This "loginattempts" login is used to predict whether IP is performing DOS attack or not. For this , only those IP's are considered where after they log in , they don'e execute any commands and just leave. Spark K-Means Machine learning algorithm has been used to divide the IP's into 2 categories "High" and "Low". CSV file is generated  which is supplied as input to K-Means algorithm. Pseudo code is given below.

\lstinputlisting[language=python, caption=Generate Login Attempts in a CSV File, captionpos=top]{./files/04_GenerateLoginAttemptsCSV.py}

\lstinputlisting[language=python, caption=Applying Spark K-Means Algorithm, captionpos=top]{./files/05_K-Means.py}

\lstinputlisting[language=python, caption=Counting of DOS attacks, captionpos=top]{./files/06_countDOSAttacks.py}

\paragraph{}
After the above python file is executed, DOS attacks table looks like below.
Some IP's are also performing sniffing attacks where in they login and check for busybox utility.

\begin{table}[H]
\begin{tabular}{ |p{2cm}|p{5cm}|p{5cm}|  }
 \hline
 \textbf{S.No.} & \textbf{DOS Cluster} & \textbf{No. of IP's}\\
 \hline
 \hline
 1 & Severe & 16 \\
 \hline
2 & Medium & 5 \\
\hline
3 & Low & 1842 \\
\hline
\hline
 & \textbf{Total} & \textbf{1863} \\
\hline
\end{tabular}
 \caption{DOS Attacks Clustering Table}
\end{table}

\subsubsection{Modified Naive Bayes With K-Means for Classification}
\paragraph{}
Based on the commands executed by the IP, they have been classified into two classes as high and low using Modified Naive Bayes. As all the data is unlabeled , Sentiment analysis cannot be applied using original Naive Bayes.In modified naive Bayes, prior probability is assumed to be one as we have all data which belongs to only attack class. Effective algorithm is given below.

\lstinputlisting[language=python, caption=Modified Naive Bayes Algorithm Code, captionpos=top]{./files/07_Sentiment_Analysis.py}

\paragraph{}
After the application of Modified Naive Bayes, Posterior Probabilities are generated which are given as input to K-Means algorithm which groups them into clusters. Clusters are designed to be of two types naming high and low.
Based on the results, below table has been deduced.

\lstinputlisting[language=python, caption=K-Means Algorithm using Spark, captionpos=top]{./files/08_Sentiments_K_means.py}

\lstinputlisting[language=python, caption=Classifying IPs, captionpos=top]{./files/09_ClassifyIPs.py}





\begin{algorithm}[H]
\caption{Modified Naive Bayes With K-Means Clustering Algorithms}\label{euclid}
\begin{algorithmic}[1]

\State $\textbf{Probability}\textit{(IP is Malicious / Commands executed by IP }\text{) = }$
\begin{equation*}
  		\frac{\textbf{Probability}\textit{(IP is Malicious) * }\textbf{Probability}\textit{(Commands executed by IP / IP is Malicious)}}{\textbf{Probability}\textit{(Commands executed by IP)}}
	\end{equation*}
\State $\textbf{Note :}\textit{As we are working on Unclassified Data,We are assuming below values.}$
\State $\textbf{Probability}\textit{(IP is Malicious) = }\textbf{1} $
\State $\textbf{Probability}\textit{(Commands executed by IP) = }\textbf{1} $
\State $\textbf{Probability}\textit{(Commands executed by IP / IP is Malicious) = }$

\begin{equation*}
  		\frac{\textit{Frequency of Word + 1 (Smoothening Factor)}}{\textit{ Total Number of Words + Unique Number of Words}}
	\end{equation*}

\State $\textbf{Note :}\textit{ Commands executed by each IP is split into words and their }$
$\centerline{\textit{frequencies are stored in Database}}$
\State $\textit{Total No of Words} \gets \textit{N}$
\State $\textit{Unique No of Words} \gets \textit{dict}$
\While {$\textit{IP List} \textbf{ not } \textit{Empty} $}
	\State $\textit{No of Words} \gets \textit{Splitting Commands into words}$
	\State $\textbf{Probability}\textit{(IP is Malicious / Commands executed by IP }\text{)}$
	\State $\textbf{= Probability (}\textit{Commands executed by IP / IP is Malicious}) $ 
	\State $\textbf{= }\prod_{i=1}^{\textit{No of Words}}\textbf{Probability}\textit{(}\textit{Word}_i\textit{ in Command Executed/ IP is Malicious)}$
	\State $\textbf{= }\prod_{i=1}^{\textit{No of Words}}\frac{\textit{n}_i\textit{+ 1 }}{\textit{ N + dict}}$	
	
\EndWhile
\State $\textbf{done}$
\State $\textbf{Note : }\textit{ Each IP Final Probability will be stored in Database}$

\State $\textbf{Note : }\textit{After Storing the IP's Probabilities in Database,}\textbf{K-Means} $ $\textit{Clustering algorithm is applied to classify the IP's into two classes as high and Low.}$
 $\centerline{***********************************}$
 $\textbf{\centerline{K-Means is given below}}$
 $\centerline{***********************************}$
\State $\textbf{Input : }\textit{Set of Data Points }\textbf{D}\textit{ and No. of Clusters}\textbf{ K.}$
\State $\textbf{Output : }\textit{Cluster Centers that minimizes the squared error distortion.}$
\State $\textbf{Algorithm : }$
\begin{enumerate}
   \item \textit{Pick K Data points randomly from D to form cluster centers.}
   \item \textit{Assign each data point to its nearest cluster center by calculating and taking the minimum Euclidean Squared distance metric.}
   \item \textit{After all data points are assigned to their clusters,move each cluster center to mean of its assigned data points.}
   \item \textit{With new cluster centers, repeat 2 to 3 until there is no convergence.}
\end{enumerate}
\end{algorithmic}
\end{algorithm}


\begin{table}[H]
\begin{tabular}{ |p{2cm}|p{5cm}|p{5cm}|  }
 \hline
 \textbf{S.No.} & \textbf{Cluster Name} & \textbf{No. of IP's}\\
 \hline
 \hline
 1 & High & 136 \\
 \hline
2 & Low & 1842 \\
\hline
\hline
 & \textbf{Total} & \textbf{1978} \\
\hline
\end{tabular}
 \caption{Modified Naive Bayes and K-Means Results}
\end{table}

\subsubsection{DDOS Binaries}
\paragraph{}
Attackers who were able to login successfully has used their access to full effect by deploying the binaries and executable files on the machine and running them remotely whenever needed. Fortunately Cowrie happened to be a medium interaction honeypot which would not allow running any binaries or executable files. Some of the captured binaries are sent to VirusTotal and all of them are identified as serious DDos attacking binaries. Some of them are listed below.



\begin{table}[h!]
\begin{center}

\begin{tabular}{ |c|c|c|c| } 
 \hline
 ./lPg5Am8r & ./AObM55mP & ./ScDrDSSt & ./YL88yLHr\\
 \hline
 ./Tr5l603l & ./yu5LvV97 & ./VdPacLUl  & ./uzsiO4Hx \\
 \hline
 ./BRB3bpfb  & ./PWnQ7Tcn  & ./GgzxWgHv & ./zanTB34C  \\
 \hline
 ./udp4858 &  ./kEbIZq9x & ./MvlFggnh & ./6PcycAuj \\
 \hline
 ./o3e1ROxG  & ./b1fNqt0C & ./iUk3up10 & ./M7vu30G4 \\
 \hline
 ./KAFsqrpv & ./GYOIsFtx & ./tf2SD7fn &  \\
 \hline
\end{tabular}
\caption{List of DDos Binaries}
\end{center}
\end{table}

\paragraph{}
Some of the malicious URL's accessed by the attackers is captured by the cowrie. These are given below in the table.


\begin{table}[h!]
\begin{center}

\begin{tabular}{ |c| } 
 \hline
 http://www.bizqsoft.com/tp2/r6.log\\
 \hline
 http://185.165.29.196/lmao.sh\\
 \hline
 http://mdb7.cn:8081/exp  \\
 \hline
 http://203.146.208.208/drago/images/.ssh/y.txt \\
 \hline
 http://162.248.4.130/isu80 \\
 \hline
 https://doiaru.000webhostapp.com/zeni.txt  \\
 \hline
 http://61.147.112.10:1242/udp4858  \\
 \hline
 http://104.216.151.157/i3306m  \\
 \hline
  http://23.228.113.240/do3309  \\
 \hline
  http://23.228.113.240/ys808e  \\
 \hline
  http://14.142.118.25/m.sh  \\
 \hline
  http://107.189.130.20/ys808e  \\
 \hline
  http://192.200.218.166/g3308l  \\
 \hline
 
 
\end{tabular}
\caption{List of Malicious URLs}
\end{center}
\end{table}


\paragraph{}
Based on all the data listed above and tables created in PostgreSQL database and API's from VirusTotal, an intrusion detection system software has been developed which continously monitors the network where commands executed by any IP is noted and sent to the database built. If it is found in the database, it will be reported as malicious. If no information is found about the data in database, it keeps that in a separate column and keeps monitoring. If any anomaly is found, then the database will be updated with the new one. So it is a continous learning process for the software. Code is given below.

\lstinputlisting[language=python, caption=Intrusion Detection System Code, captionpos=top]{./files/16_IDS.py}

\begin{figure}[h!]
\centering
\caption{IP Identified as NOT Malicious by IDS}
\includegraphics[scale=0.7]{IP_not_Malicious_IDS.png}
\end{figure}

\begin{figure}[h!]
\centering
\caption{IP Identified as Malicious by IDS}
\includegraphics[scale=0.6]{IP_malicious_IDS.png}
\end{figure}

\paragraph{}
Based on the output from the IDS, firewall rules will be updated with the specific IP as blocking or nonblocking.

\paragraph{}
\textbf{Blocking : }\textit{iptables -A INPUT -s 192.200.218.166 -j DROP}

\paragraph{}
\textbf{Unblocking : }\textit{iptables -D INPUT -s 192.200.218.165 -j DROP}

\section{Conclusion}
In this report, we were able to figure out how vulnerable most of the internet devices are and what must be done in order to secure those devices. Recent developements in IOT has forced our hand to look at the security features at a microscopic level and improvements to be done. We have also tried to classify the different types of attacks on different IOT devices and on different ports. We have also tried to design a detection layer called Intrusion Detection System using all the historical and present data and were able to prevent if not detect some of those attacks.A lot has to be done in the security related work and hope everyone will put their hand towards acheiving it.


\begin{thebibliography}{9}



\bibitem{REF-1}
  Performance of ELK Stack and Commercial System in Security Log Analysis by \textit{ Sung Jun Son and Youngmi Kwon}
 
\bibitem{REF-2}
Building an IoT Data Hub with Elasticsearch, Logstash and Kibana by \textit{ Marcin Bajer}

\bibitem{REF-3}
Network security enhancement through effective log analysis using ELK by \textit{ Ibrahim Yahya Mohammed Al-Mahbashi and M. B. Potdar and Prashant Chauhan}

\bibitem{REF-4}

Malware capturing and detection in dionaea honeypot by \textit{ P Dilsheer Ali and T. Gireesh Kumar}

\bibitem{REF-5}

Integration of network intrusion detection systems and honeypot networks for cloud security by \textit{ Varan Mahajan and Sateesh K Peddoju}


\bibitem{REF-6}
  Honeynet Data Analysis and Distributed SSH Brute-Force Attacks by \textit{Gokul Kannan Sadasivam and Chittaranjan Hota and Anand Bhojan}
  
\bibitem{REF-7}
  Detection of Severe SSH Attacks using Honeypot Servers and Machine Learning Techniques by \textit{Gokul Kannan Sadasivam and Chittaranjan Hota and Anand Bhojan S}


\bibitem{REF-8}
  Classification of SSH attacks using Machine learning algorithms  by \textit{Gokul Kannan Sadasivam and Chittaranjan Hota and and Anand Bhojan}
  
\bibitem{REF-9}

IoT honeypot: A multi-component solution for handling manual and Mirai-based attacks by \textit{ Sasa Mrdovic and Haris Šemić}

\bibitem{REF-10}

Analysis of modern intrusion detection system by \textit{ Aleksey A. Titorenko and Alexey A. Frolov}

\bibitem{REF-11}
Clustering Approach Based on Mini Batch Kmeans for Intrusion Detection System Over Big Data by \textit{ Parisa Lotfallahtabrizi and Yasser Morgan}

\bibitem{REF-12}
Feature selection based intrusion detection system using the combination of DBSCAN, K-Mean++ and SMO algorithms by \textit{ Vandana Shakya and Rajni Ranjan Singh Makwana}


\bibitem{REF-13}

DDoS Attack Detection System: Utilizing Classification Algorithms with Apache Spark by \textit{ Amjad Alsirhani and Srinivas Sampalli and Peter Bodorik}

\bibitem{REF-14}

Testing of algorithms for anomaly detection in Big data using apache spark by \textit{ Sheeraz Niaz Lighari and Dil Muhammad Akbar Hussain}

\bibitem{REF-15}
Feature selection based intrusion detection system using the combination of DBSCAN, K-Mean++ and SMO algorithms by \textit{ Vandana Shakya and Rajni Ranjan Singh Makwana}


\bibitem{REF-16}

https://www.virustotal.com

\bibitem{REF-17}


Privacy and Security of Cloud-Based Internet of Things (IoT) by \textit{ Tanupriya Choudhury and Ayushi Gupta and Saurabh Pradhan and Praveen Kumar; Yogesh Singh Rathore}

\bibitem{REF-18}
Internet of Things (IoT) security platforms by \textit{ Ibrahim R. Waz and Mohamed Ali Sobh and Ayman M. Bahaa-Eldin}

\bibitem{REF-19}
A Search Engine Backed by Internet-Wide Scanning by \textit{Zakir Durumeric and David Adrian and Ariana Mirian and  Michael Bailey and J. Alex Halderman}

\end{thebibliography}








\end{document}

